{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib tk\n",
    "# %matplotlib qt\n",
    "import matplotlib.pyplot as plt\n",
    "plt.xkcd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-Learning Agent (Jax/Flax)\n",
    "==========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "from flax import nnx\n",
    "\n",
    "class LanderNetwork(nnx.Module):\n",
    "    def __init__(self, layers=None, rngs: nnx.Rngs=None) -> None:\n",
    "        if rngs is None: rngs = nnx.Rngs(0)\n",
    "        if layers is None: layers = [8, 128, 128, 4]\n",
    "        layers = [[nnx.Linear(i, o, rngs=rngs), nnx.relu] for i, o in zip(layers[:-1], layers[1:])]\n",
    "        layers = sum(layers, [])[:-1]\n",
    "        self.sequential = nnx.Sequential(\n",
    "            *layers\n",
    "            # nnx.Linear(8, 128, rngs=rngs),\n",
    "            # nnx.relu,\n",
    "            # nnx.Linear(128, 128, rngs=rngs),\n",
    "            # nnx.relu,\n",
    "            # nnx.Linear(128, 4, rngs=rngs)\n",
    "        )\n",
    "    def __call__(self, x):\n",
    "        return self.sequential(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "ln = LanderNetwork()\n",
    "ln(np.random.randn(8)).argmax(-1).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnx.display(LanderNetwork([8, 512, 256, 4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import Agent\n",
    "from collections import deque\n",
    "import optax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "class TrainableAgent(Agent):\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        network: nnx.Module, \n",
    "        buffer_size=32, batch_size=32,\n",
    "        gamma=0.9, \n",
    "        lr=0.001, \n",
    "        train_interval=4, \n",
    "        epsilon=0.1\n",
    "        ) -> None:\n",
    "        \n",
    "        self.network = network\n",
    "        self._training = False\n",
    "        self.buffer = deque([], maxlen=buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.train_interval = train_interval\n",
    "        self.training_steps = 0\n",
    "        self.gamma = gamma\n",
    "        self.losses = []\n",
    "        self.epsilon=epsilon\n",
    "        \n",
    "        self.optim = nnx.Optimizer(self.network, optax.adamw(lr,))\n",
    "        # self.gradient_on_samples = nnx.jit(nnx.grad(TrainableAgent.evaluate_samples))\n",
    "        self.gradient_on_samples = nnx.jit(nnx.grad(TrainableAgent.batch_evaluate_samples))\n",
    "        \n",
    "        \n",
    "    def __call__(self, x) -> np.ndarray:\n",
    "        return self.network(x)\n",
    "    def train(self):\n",
    "        self.network.train()\n",
    "        self._training=True\n",
    "        return self\n",
    "    def eval(self):\n",
    "        self.network.eval()\n",
    "        self._training=False\n",
    "        return self\n",
    "    def act(self, observation, periphral=None):\n",
    "        if self._training and np.random.rand() < self.epsilon:\n",
    "            return self._explore(observation)\n",
    "        return self._exploit(observation)\n",
    "    def _explore(self, observation):\n",
    "        return np.random.randint(4)\n",
    "    def _exploit(self, observation):\n",
    "        return self.network(observation).argmax(-1).item()\n",
    "    \n",
    "    def record_observation(self, observation, action, reward, next_observation, terminated):\n",
    "        if not self._training:\n",
    "            return\n",
    "        self.buffer.append((observation, action, reward, next_observation, int(terminated)))\n",
    "        self.training_steps+=1\n",
    "        self.losses.append(self.evaluate_sample(self.network, self.gamma, *self.buffer[-1]))\n",
    "        \n",
    "        if self.training_steps%self.train_interval==0 and len(self.buffer) > self.batch_size:\n",
    "            batch = [self.buffer[i] for i in np.random.choice(np.arange(len(self.buffer)), self.batch_size)]\n",
    "            self.train_on_samples(self.network, batch)\n",
    "\n",
    "    def train_on_samples(self, network, samples):\n",
    "        grads = self.gradient_on_samples(network, self.gamma, samples)\n",
    "        self.optim.update(grads)\n",
    "    \n",
    "    @staticmethod\n",
    "    def evaluate_samples(network, gamma, samples):\n",
    "        return sum((TrainableAgent.evaluate_sample(network, gamma, *sample) for sample in samples))\n",
    "    \n",
    "    @staticmethod\n",
    "    def batch_evaluate_samples(network: nnx.Module, gamma:int, samples):\n",
    "        observations, actions, rewards, next_observations, terminateds = zip(*samples)\n",
    "        terminateds = jnp.array(terminateds)\n",
    "        rewards = jnp.array(rewards)\n",
    "        cr_preds = jnp.stack([pred[a] for pred, a in zip(network(observations), actions)])\n",
    "        cr_estimates = rewards + (1-terminateds)*gamma*network(next_observations).argmax(-1)\n",
    "        return sum((cr_preds-cr_estimates)**2)\n",
    "\n",
    "    @staticmethod\n",
    "    def evaluate_sample(network, gamma, observation, action, reward, next_observation, terminated):\n",
    "        cr_pred = network(observation)[action]\n",
    "        cr_estimate = reward + (1-terminated)*gamma*network(next_observation).argmax(-1)\n",
    "        return (cr_pred-cr_estimate)**2\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single demo run\n",
    "---------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_one_run\n",
    "import gymnasium as gym\n",
    "enviz = gym.make(\"LunarLander-v3\", render_mode='human')\n",
    "tna = TrainableAgent(LanderNetwork()).eval()\n",
    "plot_one_run(enviz, tna, )\n",
    "enviz.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()\n",
    "enviz.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training\n",
    "--------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import gymnasium as gym\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "tna = TrainableAgent(LanderNetwork([8, 256, 256, 4]), buffer_size=50_000, gamma=0.99, lr=0.0005, batch_size=32, train_interval=1)\n",
    "cont = None\n",
    "runs = [[]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [26:14<00:00, 63.53it/s]\n"
     ]
    }
   ],
   "source": [
    "from utils import run_upto_n_steps, plot_reward_and_episodes, plot_one_run\n",
    "from tqdm import trange\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_losses_and_runs(losses, runs):\n",
    "    plt.clf()\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.scatter(*zip(*enumerate(losses)), )\n",
    "    plt.yscale('log')\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plot_reward_and_episodes(runs[:-1]) if len(runs) > 1 else None\n",
    "    plt.pause(0.5)\n",
    "\n",
    "def demo_one_run(agent: Agent):\n",
    "    plt.clf()\n",
    "    agent.eval()\n",
    "    enviz = gym.make(\"LunarLander-v2\", render_mode='human')\n",
    "    plot_one_run(enviz, agent, )\n",
    "    enviz.close()\n",
    "    agent.train()\n",
    "tna.train()\n",
    "\n",
    "step_limp = tna.training_steps + 100_000\n",
    "prog_bar = trange(tna.training_steps, step_limp)\n",
    "plots=0\n",
    "demos=0\n",
    "\n",
    "try:\n",
    "    while tna.training_steps < step_limp:\n",
    "        init_steps = tna.training_steps\n",
    "        cont, runs = run_upto_n_steps(env, tna, 1_000, cont, runs)\n",
    "        prog_bar.update(np.min([tna.training_steps, step_limp]) - init_steps, )\n",
    "\n",
    "        if tna.training_steps // 5_000 > demos:\n",
    "            demo_one_run(tna)\n",
    "            demos = tna.training_steps//5_000\n",
    "\n",
    "        if tna.training_steps // 500 > plots:\n",
    "            plot_losses_and_runs(tna.losses, runs)\n",
    "            plots = tna.training_steps//500\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "prog_bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "prog_bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tna.training_steps, len(tna.losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_one_run(tna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses_and_runs(tna.losses, runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LanderNetwork' object has no attribute 'params'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[65], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mflax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mserialization\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m to_state_dict\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# to_state_dict(tna.network.params)\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[43mtna\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnetwork\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# state = TrainState.create(\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# apply_fn=tna.network.__call__\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# tna.network.__call__\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# tna.network\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# tna.optim\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LanderNetwork' object has no attribute 'params'"
     ]
    }
   ],
   "source": [
    "# from flax.training.train_state import TrainState\n",
    "from flax.serialization import to_state_dict\n",
    "\n",
    "# to_state_dict(tna.network.params)\n",
    "tna.network.params\n",
    "\n",
    "\n",
    "# state = TrainState.create(\n",
    "    # apply_fn=tna.network.__call__\n",
    "# )\n",
    "# tna.network.__call__\n",
    "# tna.network\n",
    "# tna.optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "State({\n",
       "  'sequential': {\n",
       "    'layers': {\n",
       "      0: {\n",
       "        'bias': VariableState(\n",
       "          type=Param,\n",
       "          value=Array([ 0.13557155,  0.30685747, -0.23246528, -0.01823224, -0.35369515,\n",
       "                 -0.06348848, -0.01729271, -0.26939735, -0.11670183, -0.37542927,\n",
       "                 -0.35553658, -0.2566189 ,  0.17960155,  0.08465502, -0.10438312,\n",
       "                  0.11032265, -0.38376758, -0.34675905, -0.31562647, -0.0350286 ,\n",
       "                 -0.30342135, -0.06824041,  0.45229772, -0.28726396, -0.5977671 ,\n",
       "                 -0.08031852,  0.06493102, -0.0762317 ,  0.6056754 ,  0.07038745,\n",
       "                 -0.5302734 , -0.25186   , -0.03937203, -0.23438355, -0.27541783,\n",
       "                  0.01824808, -0.23723437,  0.11527891, -0.34089193,  0.35463247,\n",
       "                  0.00333621, -0.29955506,  0.05605226,  0.56325483, -0.16099413,\n",
       "                  0.28652623, -0.5706288 ,  0.15694661, -0.33608243, -0.11778982,\n",
       "                  0.2143858 , -0.12337278, -0.19445583, -0.40483987,  0.01599739,\n",
       "                  0.41289443, -0.16334039, -0.15845776, -0.10754589, -0.20453912,\n",
       "                 -0.3198954 ,  0.10660306, -0.10264225,  0.02248297, -0.2379143 ,\n",
       "                  0.21813995, -0.18196887, -0.33863586, -0.35134077, -0.15184341,\n",
       "                 -0.09380776, -0.27405688,  0.3695756 ,  0.31350568, -0.3808455 ,\n",
       "                 -0.03413749,  0.3231667 , -0.16517468, -0.25312978,  0.34845415,\n",
       "                 -0.02923356,  0.016849  ,  0.0506718 , -0.3425983 , -0.02623715,\n",
       "                  0.01940571, -0.32018656,  0.18060191,  0.00971167, -0.3367853 ,\n",
       "                 -0.10350366, -0.25314096,  0.08538784, -0.24808711,  0.42324767,\n",
       "                  0.16454807, -0.02448173, -0.41788438,  0.60841644,  0.09657583,\n",
       "                  0.1923313 ,  0.21811259, -0.24404262, -0.3730789 ,  0.21401134,\n",
       "                  0.5542241 ,  0.27309054, -0.2976202 , -0.2169854 ,  0.16857195,\n",
       "                  0.6119166 ,  0.2516551 , -0.06530142, -0.03559117, -0.3123385 ,\n",
       "                 -0.13890524,  0.09092782,  0.00418259, -0.09800237, -0.13579093,\n",
       "                 -0.3434951 , -0.3343953 , -0.13198827, -0.13664462,  0.24422954,\n",
       "                  0.09979758, -0.19840431, -0.52731025,  0.2516859 ,  0.02457092,\n",
       "                  0.02467948, -0.20778032, -0.06657389, -0.007914  , -0.07431175,\n",
       "                 -0.16966581, -0.22969723, -0.01101046, -0.2665388 ,  0.25293761,\n",
       "                 -0.04831067,  0.31990904, -0.13146998,  0.0368215 , -0.27012378,\n",
       "                 -0.22911195,  0.14039318, -0.11010614, -0.2512977 ,  0.21884295,\n",
       "                  0.22168149,  0.2999254 ,  0.07054596, -0.10678633,  0.02462561,\n",
       "                 -0.17621824,  0.08274063,  0.36890692, -0.33729222, -0.25951743,\n",
       "                  0.2577237 , -0.01798824, -0.01977974, -0.06142657,  0.00139884,\n",
       "                 -0.18652403,  0.04389971,  0.442481  ,  0.12520853, -0.02956796,\n",
       "                 -0.03098964,  0.06337251,  0.19334179,  0.39711997,  0.09271094,\n",
       "                 -0.16151999, -0.28134525, -0.1021252 , -0.15182766, -0.27050114,\n",
       "                  0.49752128, -0.2281974 , -0.3293928 , -0.19249019,  0.3662663 ,\n",
       "                 -0.06989499,  0.15276158,  0.33968332, -0.02374737, -0.24233691,\n",
       "                 -0.13328324, -0.365637  , -0.0571292 , -0.17590998, -0.27942812,\n",
       "                 -0.41384032,  0.06069505, -0.05351   , -0.16956304,  0.32180843,\n",
       "                  0.2784266 ,  0.12633504, -0.29978228,  0.01553535, -0.36591715,\n",
       "                  0.39183465, -0.15895154, -0.17691956, -0.46343908, -0.11856636,\n",
       "                 -0.19267431,  0.19296415,  0.4422348 ,  0.05553757, -0.48276967,\n",
       "                 -0.09257101,  0.00387654, -0.31508973, -0.51094866, -0.09123881,\n",
       "                 -0.25084344, -0.03718573, -0.27082902,  0.32697147, -0.0531779 ,\n",
       "                 -0.15014808, -0.17699565, -0.44069862, -0.04682732, -0.24864656,\n",
       "                  0.07685477, -0.22404419,  0.0391049 ,  0.15899368, -0.02903218,\n",
       "                  0.01669629, -0.17467196, -0.16080455, -0.09526689, -0.07998247,\n",
       "                  0.09421045, -0.58885247, -0.04492423,  0.31610808, -0.29310372,\n",
       "                 -0.03575889,  0.14868638,  0.05123314, -0.31195432, -0.44097057,\n",
       "                 -0.11640869, -0.16817877,  0.03152975,  0.08616017, -0.23748733,\n",
       "                  0.33325908], dtype=float32)\n",
       "        ),\n",
       "        'kernel': VariableState(\n",
       "          type=Param,\n",
       "          value=Array([[-0.4146112 ,  0.20862004, -0.40984362, ..., -0.06574641,\n",
       "                   0.0582897 , -0.3180324 ],\n",
       "                 [-0.5241968 , -1.4335145 ,  0.00252046, ..., -0.03271971,\n",
       "                  -1.5152316 , -0.18380275],\n",
       "                 [ 0.2043142 , -0.77345383, -0.01426687, ...,  0.6153644 ,\n",
       "                   0.9120349 , -0.5411359 ],\n",
       "                 ...,\n",
       "                 [ 0.61507416, -0.7551085 , -0.6056554 , ..., -0.08564312,\n",
       "                  -0.92175335, -0.48927695],\n",
       "                 [-0.05823007,  0.23814061, -0.3516204 , ...,  0.7706676 ,\n",
       "                   0.20459114,  0.5895166 ],\n",
       "                 [-0.57931834, -0.31963187,  0.9221795 , ...,  0.5312238 ,\n",
       "                  -0.19975358,  0.02406051]], dtype=float32)\n",
       "        )\n",
       "      },\n",
       "      2: {\n",
       "        'bias': VariableState(\n",
       "          type=Param,\n",
       "          value=Array([ 6.10430576e-02, -4.81441729e-02, -1.05444796e-01, -7.09054172e-02,\n",
       "                 -1.79835066e-01,  7.82675669e-02, -3.89995277e-02, -3.72529030e-02,\n",
       "                 -8.44918564e-03, -1.02083802e-01,  3.46523002e-02, -6.00343756e-02,\n",
       "                 -1.57409925e-02, -4.26238328e-02, -3.34033296e-02,  1.84302643e-01,\n",
       "                 -1.86264515e-02, -9.03232172e-02, -2.65396833e-02, -1.82362925e-02,\n",
       "                 -1.40846306e-02, -1.60284460e-01, -2.18851820e-01, -1.21871859e-01,\n",
       "                 -1.43431097e-01, -5.49411848e-02, -7.64374994e-03, -8.13320559e-03,\n",
       "                 -1.95881695e-01,  2.85873972e-02, -2.49160439e-01, -6.65039569e-02,\n",
       "                 -1.53741345e-01, -1.58684030e-01, -1.38569698e-01, -2.59156469e-02,\n",
       "                  1.11871555e-01, -3.13915908e-02, -1.87535152e-01,  1.46359980e-01,\n",
       "                 -2.15211958e-01, -7.02418238e-02, -1.03948191e-01, -1.35878306e-02,\n",
       "                 -2.42650583e-02, -1.01916008e-01, -7.11843520e-02, -2.10682489e-02,\n",
       "                 -5.30489758e-02,  2.77844872e-02,  1.58039685e-02, -1.86264515e-02,\n",
       "                  3.90808523e-01, -2.95534104e-01, -8.09534919e-03, -2.57672697e-01,\n",
       "                  1.77635282e-01,  3.41700107e-01,  5.59854768e-02, -1.92447990e-01,\n",
       "                  8.77625681e-03, -2.89156526e-01,  2.77584940e-01, -5.53818420e-03,\n",
       "                 -1.28043413e-01, -1.43830329e-02,  8.76098722e-02, -1.95021823e-01,\n",
       "                 -1.56436469e-02,  2.04455599e-01, -8.34132209e-02, -1.90048106e-02,\n",
       "                 -7.22258724e-03, -9.40016136e-02,  1.34747609e-01, -4.45810966e-02,\n",
       "                 -1.48475707e-01, -1.36574760e-01, -2.68067755e-02, -5.04725613e-03,\n",
       "                 -1.22266091e-01, -3.17369327e-02, -2.28287160e-01, -7.94369876e-02,\n",
       "                 -2.84874924e-02, -1.95401907e-02, -2.56954819e-01, -4.21403050e-02,\n",
       "                 -3.87713388e-02, -1.94207653e-01, -1.98025227e-01, -2.83445626e-01,\n",
       "                  2.15051826e-02,  7.64835924e-02,  9.94241163e-02,  2.11826697e-01,\n",
       "                 -2.97069158e-02, -6.63627777e-03, -2.87468940e-01,  2.85356551e-01,\n",
       "                 -1.29262522e-01, -2.72829775e-02,  4.15376015e-02,  8.90202075e-03,\n",
       "                  2.01045210e-03, -9.77927819e-02,  1.93820551e-01, -1.02781048e-02,\n",
       "                 -1.71466127e-01,  1.03050701e-01,  1.61188230e-01, -4.45155948e-02,\n",
       "                 -1.14638787e-02,  1.08244695e-01, -1.38665726e-02, -9.94148944e-03,\n",
       "                  2.00051397e-01, -1.41037270e-01, -1.42125532e-01,  1.47711430e-02,\n",
       "                  1.27816349e-01, -2.23132707e-02, -9.56944525e-02, -6.85657412e-02,\n",
       "                  3.21601242e-01, -1.22778855e-01,  1.29857987e-01,  1.68026730e-01,\n",
       "                 -8.60699117e-02, -4.17881907e-04, -7.32384175e-02, -1.25266805e-01,\n",
       "                  3.71224713e-03,  9.98989195e-02, -1.51207373e-01, -1.40305012e-01,\n",
       "                 -3.04966383e-02,  2.30465665e-01, -9.38878395e-03,  2.30439395e-01,\n",
       "                 -2.95667704e-02, -8.38969499e-02, -4.64149527e-02,  1.75850734e-01,\n",
       "                 -1.32392600e-01, -2.79732823e-01,  3.85319651e-03, -6.83836937e-02,\n",
       "                 -1.58820096e-02, -4.96630929e-03, -2.90226229e-02,  1.91947855e-02,\n",
       "                  1.40978128e-01, -2.62323674e-02, -1.86245322e-01, -2.23944515e-01,\n",
       "                  4.72304821e-01,  1.03976727e-01, -1.66810587e-01, -2.41662949e-01,\n",
       "                  1.32744044e-01, -3.65013741e-02, -2.68951152e-02, -2.06895217e-01,\n",
       "                 -2.44186427e-02, -3.50756533e-02, -2.68530995e-01, -5.40522560e-02,\n",
       "                  3.74534391e-02,  1.04356863e-01,  7.45396242e-02, -4.20867279e-02,\n",
       "                 -4.75105755e-02,  2.71857500e-01,  2.16494769e-01, -1.09911442e-01,\n",
       "                  9.18036606e-03, -2.18562856e-02,  1.82330403e-02, -4.20979932e-02,\n",
       "                 -3.82030420e-02, -2.68997848e-02, -6.57386035e-02, -1.30536497e-01,\n",
       "                 -1.83543444e-01, -2.25534812e-02,  1.53120175e-01,  1.38066098e-01,\n",
       "                 -4.04993705e-02, -2.32519999e-01, -2.05701627e-02, -1.30256563e-01,\n",
       "                 -2.03270331e-01, -1.01554342e-01,  1.17769547e-01, -3.26964036e-02,\n",
       "                  1.18290238e-01,  4.24304679e-02,  2.06945911e-01, -5.91618381e-02,\n",
       "                 -6.03536107e-02, -2.05631815e-02, -2.85353959e-02, -3.57529595e-02,\n",
       "                  2.49814495e-01,  8.03353786e-02, -1.12275770e-02, -1.28658023e-02,\n",
       "                 -7.60049969e-02, -1.34479359e-01, -9.91342515e-02, -5.85166039e-03,\n",
       "                 -4.64647934e-02, -1.74474176e-02, -1.03914469e-01,  1.23334952e-01,\n",
       "                  2.80596614e-01,  4.56231944e-02,  1.32640839e-01, -1.50941193e-01,\n",
       "                 -1.54594213e-01,  4.18994529e-03, -3.87081057e-02, -2.29130201e-02,\n",
       "                 -3.13468687e-02, -2.86256939e-01,  1.37017205e-01,  4.39852178e-02,\n",
       "                 -3.15199383e-02, -1.54902115e-01, -2.40001410e-01, -5.99113032e-02,\n",
       "                 -3.73140365e-01,  2.69925982e-01, -4.44092661e-01,  4.93225344e-02,\n",
       "                 -1.60778999e-01,  1.65591806e-01, -1.22779764e-01,  6.13218965e-03,\n",
       "                 -3.56549621e-01,  5.00889234e-02, -1.82395820e-02, -4.92295474e-02,\n",
       "                 -2.63599139e-02, -8.77209902e-02, -1.87259670e-02,  3.69342864e-01,\n",
       "                 -1.38867155e-01, -2.69721389e-01, -1.01920376e-02, -2.88651371e-03,\n",
       "                 -1.91676050e-01, -7.81273842e-02,  1.79291651e-01, -8.47792719e-03],      dtype=float32)\n",
       "        ),\n",
       "        'kernel': VariableState(\n",
       "          type=Param,\n",
       "          value=Array([[ 0.13918146,  0.00442589,  0.17685597, ...,  0.07363364,\n",
       "                   0.02656182, -0.00192461],\n",
       "                 [ 0.21200371,  0.15026306, -0.3515342 , ..., -0.60204124,\n",
       "                   0.14256276,  0.3838135 ],\n",
       "                 [ 0.08956716, -0.1853401 ,  0.06350534, ..., -0.18262434,\n",
       "                   0.1656336 ,  0.16475968],\n",
       "                 ...,\n",
       "                 [ 0.09666215, -0.47121674,  0.05597993, ..., -0.16306505,\n",
       "                  -0.10308039, -0.24762566],\n",
       "                 [ 0.05477597, -0.15357937,  0.09378058, ..., -0.36881435,\n",
       "                   0.02802823, -0.23497571],\n",
       "                 [-0.11939766,  0.05771276, -0.0081925 , ..., -0.4673493 ,\n",
       "                   0.04295223, -0.44128907]], dtype=float32)\n",
       "        )\n",
       "      },\n",
       "      4: {\n",
       "        'bias': VariableState(\n",
       "          type=Param,\n",
       "          value=Array([-0.07793245, -0.06619917,  0.25083622, -0.05596113], dtype=float32)\n",
       "        ),\n",
       "        'kernel': VariableState(\n",
       "          type=Param,\n",
       "          value=Array([[ 7.1405344e-02,  5.6794345e-01,  5.7312477e-01,  5.6673169e-02],\n",
       "                 [ 5.0407320e-01, -6.6593133e-02, -7.3966438e-01,  1.1497037e-01],\n",
       "                 [ 1.2668027e-01,  3.1594044e-01,  5.9126894e-04,  2.9020542e-01],\n",
       "                 ...,\n",
       "                 [-7.3969269e-01,  3.8494563e-01, -8.7249152e-02, -5.4853374e-01],\n",
       "                 [ 1.5896738e-01,  2.0754315e-01,  6.4971440e-02,  4.9740908e-01],\n",
       "                 [ 7.7337399e-02, -3.2341525e-01,  4.5809561e-01,  2.3267046e-01]],      dtype=float32)\n",
       "        )\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "})"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnx.state(tna.network)\n",
    "# nnx.split(tna.network)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-Learning Agent (pytorch)\n",
    "=========================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "setup\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "enviz = gym.make(\"LunarLander-v3\", render_mode='human')\n",
    "cont = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enviz.observation_space.sample().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enviz.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LanderNetwork(nn.Module):\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(8, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 4),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Network Agent\n",
    "--------------------\n",
    "This is just a simple agent that uses the network but does not train it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import Agent\n",
    "class SimpleNetworkAgent(Agent):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.network = LanderNetwork()\n",
    "    def act(self, observation, periphral=None):\n",
    "        obs = torch.from_numpy(observation)\n",
    "        action = self.network(obs)\n",
    "        return action.argmax().item()\n",
    "    def record_observation(self, observation_old, action, reward, observation, terminated):\n",
    "        return super().record_observation(observation_old, action, reward, observation, terminated)\n",
    "\n",
    "sna = SimpleNetworkAgent()\n",
    "cont = None\n",
    "runs = [[]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_one_run\n",
    "\n",
    "plot_one_run(enviz, sna,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "enviz.close()\n",
    "# plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "enviz = gym.make(\"LunarLander-v3\")\n",
    "cont = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import run_upto_n_steps, plot_reward_and_episodes\n",
    "from tqdm import trange\n",
    "\n",
    "for round in trange(80):\n",
    "    cont, runs = run_upto_n_steps(enviz, sna, 150, cont, runs)\n",
    "    if round % 2 == 0:\n",
    "        plt.clf()\n",
    "        plot_reward_and_episodes(runs)\n",
    "        plt.pause(0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainable Network Agent\n",
    "-----------------------\n",
    "This agent will train the network by optimizing on the most recent few \n",
    "(Observaation, Action, Reward, Observation) quartuples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "env = gym.make(\"LunarLander-v3\")\n",
    "cont = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils import Agent\n",
    "class TrainableNetworkAgent(Agent):\n",
    "    def __init__(self, gamma = 0.9, lr=0.001, update_interval=32, epsilon=0.1) -> None:\n",
    "        super().__init__()\n",
    "        self.network = LanderNetwork()\n",
    "        self.gamma = gamma\n",
    "        self.update_interval = update_interval\n",
    "        self.epsilon = epsilon\n",
    "        self.optim = torch.optim.AdamW(self.network.parameters(), lr)\n",
    "        self.loss = torch.tensor(0., requires_grad=True)\n",
    "        self.thoughts = 0\n",
    "        self.losses = []\n",
    "        self.events = [] # List of dictionary of events\n",
    "    def act(self, observation, periphral=None):\n",
    "        if self.network.training and torch.rand((1,)).item() < self.epsilon:\n",
    "            return torch.randint(self.network.layers[-1].out_features, (1,)).item()\n",
    "        obs = torch.from_numpy(observation)\n",
    "        with torch.no_grad():\n",
    "            action = self.network(obs)\n",
    "        return action.argmax().item()\n",
    "    def record_observation(self, observation, action, reward, observation_next, terminated):\n",
    "        if not self.network.training:\n",
    "            return \n",
    "        self.thoughts += 1\n",
    "        if isinstance(observation, np.ndarray):\n",
    "            observation = torch.from_numpy(observation)\n",
    "        if isinstance(observation_next, np.ndarray):\n",
    "            observation_next = torch.from_numpy(observation_next)\n",
    "        outputs = self.network(observation)\n",
    "        cr_pred = outputs[action]\n",
    "        cr_esti = reward + (self.gamma*self.network(observation_next).argmax() if not terminated else 0)\n",
    "\n",
    "        loss = (cr_esti - cr_pred)**2\n",
    "        self.loss = self.loss + loss\n",
    "        self.losses.append(loss.item())\n",
    "        self.events.append({\n",
    "            \"terminal\": terminated,\n",
    "            \"reward\": reward\n",
    "        })\n",
    "        if self.thoughts % self.update_interval == 0:\n",
    "            self.update()\n",
    "        \n",
    "    def update(self):\n",
    "        self.optim.zero_grad()\n",
    "        self.loss.backward()\n",
    "        self.optim.step()\n",
    "        self.loss = torch.tensor(0., requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "tna = TrainableNetworkAgent(gamma=0.99, lr=0.01)\n",
    "cont = None\n",
    "runs = [[]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import run_upto_n_steps, plot_reward_and_episodes\n",
    "from tqdm import trange\n",
    "from itertools import count\n",
    "tna.network.train()\n",
    "training_steps  = 10_000\n",
    "progbar = trange(training_steps)\n",
    "\n",
    "initial_steps = tna.thoughts\n",
    "breakout = False\n",
    "for round in count():\n",
    "    try:\n",
    "        cont, runs = run_upto_n_steps(env, tna, 150, cont, runs)\n",
    "        progbar.set_description(f\"thoughts {tna.thoughts}\")\n",
    "        progbar.update(tna.thoughts-initial_steps-progbar.n)\n",
    "        if progbar.n-initial_steps >= training_steps:\n",
    "            breakout = True\n",
    "        if round % 20 == 0 or breakout:\n",
    "            # plot_agent(tna)\n",
    "            plt.clf()\n",
    "            plt.subplot(2,1,1)\n",
    "            plt.scatter(*zip(*enumerate(tna.losses)), c=[i['terminal'] for i in tna.events], s=120, alpha=0.8)\n",
    "            plt.yscale('log')\n",
    "            plt.subplot(2, 1, 2)\n",
    "            plot_reward_and_episodes(runs)\n",
    "            plt.pause(0.2)\n",
    "        if round % 300 == 0 or breakout:\n",
    "            tna.network.eval()\n",
    "            enviz = gym.make(\"LunarLander-v3\", render_mode='human')\n",
    "            run_upto_n_steps(enviz, tna, 1_000)\n",
    "            enviz.close()\n",
    "            tna.network.train()\n",
    "        if breakout:\n",
    "            break\n",
    "    except KeyboardInterrupt:\n",
    "        break\n",
    "progbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_one_run\n",
    "enviz = gym.make(\"LunarLander-v3\", render_mode='human')\n",
    "tna.network.eval()\n",
    "plot_one_run(enviz, tna,)\n",
    "enviz.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.close()\n",
    "plt.suptitle(\"Simple DQN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Parallel environments\n",
    "\n",
    "This paper [Asynchronous Methods for Deep Reinforcement Learning](https://arxiv.org/abs/1602.01783) \n",
    "demonstrated a strategy of training the same agent in multiple different copies \n",
    "of the environment at the same time as a method to keep diversity in training \n",
    "data.\n",
    "\n",
    "This section is my attempt to recreate that\n",
    "\n",
    "\n",
    "steps:\n",
    "- ~~Seperate runs from agent, make run an array returned along side cont~~\n",
    "- ~~Verify nothing broke~~\n",
    "- Implement env&run swap in training loop\n",
    "- ...?\n",
    "- Profit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import TrainableNetworkAgent\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "tna = TrainableNetworkAgent(gamma=0.99, lr=0.01)\n",
    "envs = [gym.make(\"LunarLander-v3\") for _ in range(5)] \n",
    "num_envs = len(envs)\n",
    "cont = [None]*num_envs\n",
    "runs = [ [[]] ]*num_envs\n",
    "# epsilons = np.linspace(0.9, 0.1, num_envs)\n",
    "epsilons = [0.1]*num_envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import run_upto_n_steps, plot_reward_and_episodes\n",
    "from tqdm import trange\n",
    "from itertools import count\n",
    "\n",
    "tna.network.train()\n",
    "training_steps  = 80_000\n",
    "progbar = trange(training_steps)\n",
    "\n",
    "initial_steps = tna.thoughts\n",
    "breakout = False\n",
    "for round in count():\n",
    "    try:\n",
    "        enviz = round%num_envs\n",
    "        tna.epsilon = epsilons[enviz]\n",
    "        cont[enviz], runs[enviz] = run_upto_n_steps(envs[enviz], tna, 40, cont[enviz], runs[enviz])\n",
    "        progbar.set_description(f\"thoughts {tna.thoughts}\")\n",
    "        progbar.update(tna.thoughts-initial_steps-progbar.n)\n",
    "        if progbar.n-initial_steps >= training_steps:\n",
    "            breakout = True\n",
    "        if round % 20 == 0:\n",
    "            # plot_agent(tna)\n",
    "            plt.clf()\n",
    "            plt.subplot(2,1,1)\n",
    "            plt.scatter(*zip(*enumerate(tna.losses)), c=[i['terminal'] for i in tna.events], s=10, alpha=0.8)\n",
    "            plt.subplot(2, 1, 2)\n",
    "            plot_reward_and_episodes(runs[-1])\n",
    "            plt.pause(0.2)\n",
    "        if round % 500 == 0:\n",
    "            tna.network.eval()\n",
    "            enviz = gym.make(\"LunarLander-v3\", render_mode='human')\n",
    "            run_upto_n_steps(enviz, tna, 1_000)\n",
    "            enviz.close()\n",
    "            tna.network.train()\n",
    "        if breakout:\n",
    "            break\n",
    "    except KeyboardInterrupt:\n",
    "        break\n",
    "progbar.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enviz = gym.make(\"LunarLander-v3\", render_mode='human')\n",
    "tna.network.eval()\n",
    "for i in trange(5):\n",
    "    run_upto_n_steps(enviz, tna, 1_000)\n",
    "# input()\n",
    "enviz.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib tk\n",
    "# %matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import Agent, run_upto_n_steps\n",
    "import numpy as np\n",
    "class RandLander(Agent):\n",
    "    def act(self, observation, periphral=None):\n",
    "        return np.random.choice([0,1,2,3])\n",
    "    def record_observation(self, observation_old, action, reward, observation, terminated):\n",
    "        pass\n",
    "\n",
    "rand_agent = RandLander()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import Agent, run_upto_n_steps\n",
    "import gymnasium as gym\n",
    "\n",
    "enviz = gym.make(\"LunarLander-v3\", render_mode=\"human\")\n",
    "cont = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "# logging.basicConfig(level=logging.INFO) # If your interested in some logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import run_and_plot\n",
    "\n",
    "for round in range(100):\n",
    "    cont = run_and_plot(enviz, rand_agent, 50, cont)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "enviz.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "enviz = gym.make(\"LunarLander-v3\")\n",
    "cont = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import run_and_plot\n",
    "\n",
    "for round in range(100):\n",
    "    cont = run_and_plot(enviz, rand_agent, 150, cont)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "enviz = gym.make(\"LunarLander-v3\", render_mode=\"human\")\n",
    "next_observation, info = enviz.reset()\n",
    "\n",
    "episode_over = False\n",
    "while not episode_over:\n",
    "    action = enviz.action_space.sample()  # agent policy that uses the observation and info\n",
    "    next_observation, reward, terminated, truncated, info = enviz.step(action)\n",
    "\n",
    "    episode_over = terminated or truncated\n",
    "\n",
    "enviz.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define a function that takes a model and an environment, and run the model in the environment for *n* steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class Agent(ABC):\n",
    "    @abstractmethod\n",
    "    def act(self, observation, periphral=None):\n",
    "        pass\n",
    "    @abstractmethod\n",
    "    def think(self, observation_old, action, reward, observation):\n",
    "        pass\n",
    "\n",
    "def run_n_steps(env, agent: Agent, n, continuation=None):\n",
    "    if continuation is not None:\n",
    "        observation, reward, terminated, truncated, info = continuation\n",
    "    if continuation is None or terminated or truncated:\n",
    "        print(\"Resetting\")\n",
    "        observation, info = env.reset()\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        reward = 0\n",
    "    step = 0\n",
    "    # print((observation, reward, terminated, truncated, info))\n",
    "    while not terminated and not truncated and step < n:\n",
    "        action = agent.act(observation)\n",
    "        observation_old = observation\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        agent.think(observation_old, action, reward, observation)\n",
    "        step += 1\n",
    "    return (observation, reward, terminated, truncated, info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enviz = gym.make(\"LunarLander-v3\", render_mode='human')\n",
    "\n",
    "class RandomLunar(Agent):\n",
    "    def act(self, observation, periphral=None):\n",
    "        return enviz.action_space.sample()\n",
    "    def think(self, observation_old, action, reward, observation):\n",
    "        return super().think(observation_old, action, reward, observation)\n",
    "randAgent = RandomLunar()\n",
    "\n",
    "\n",
    "cont = run_n_steps(enviz, randAgent, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont = run_n_steps(enviz, randAgent, 10, cont)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "enviz.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

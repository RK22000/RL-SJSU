{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextlib.ExitStack at 0x735681fc45b0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib tk\n",
    "# %matplotlib qt\n",
    "import matplotlib.pyplot as plt\n",
    "plt.xkcd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-Learning Agent (Jax/Flax)\n",
    "==========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "from flax import nnx\n",
    "\n",
    "class LanderNetwork(nnx.Module):\n",
    "    def __init__(self, rngs: nnx.Rngs=None) -> None:\n",
    "        if rngs is None: rngs = nnx.Rngs(0)\n",
    "        self.layers = nnx.Sequential(\n",
    "            nnx.Linear(8, 128, rngs=rngs),\n",
    "            nnx.relu,\n",
    "            nnx.Linear(128, 128, rngs=rngs),\n",
    "            nnx.relu,\n",
    "            nnx.Linear(128, 4, rngs=rngs)\n",
    "        )\n",
    "    def __call__(self, x):\n",
    "        return self.layers(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(1, dtype=int32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "ln = LanderNetwork(nnx.Rngs(0))\n",
    "ln(np.random.randn(8)).argmax(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import Agent\n",
    "\n",
    "class TrainableAgent(Agent):\n",
    "    \n",
    "    def __init__(self, network: nnx.Module) -> None:\n",
    "        self.network = network\n",
    "        self._training = False\n",
    "    def training(self):\n",
    "        self.network.train()\n",
    "        self._training=True\n",
    "    def eval(self):\n",
    "        self.network.eval()\n",
    "        self._training=False\n",
    "    def act(self, observation, periphral=None):\n",
    "        if self._training:\n",
    "            return np.random.randint(4)\n",
    "        return self.network(observation).argmax(-1).item()\n",
    "    def record_observation(self, observation_old, action, reward, observation, terminated):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_one_run\n",
    "import gymnasium as gym\n",
    "env = gym.make(\"LunarLander-v3\", render_mode='human')\n",
    "plot_one_run(env, TrainableAgent(LanderNetwork()), )\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-Learning Agent (pytorch)\n",
    "=========================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "setup\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "env = gym.make(\"LunarLander-v3\", render_mode='human')\n",
    "cont = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8,)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.sample().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LanderNetwork(nn.Module):\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(8, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 4),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Network Agent\n",
    "--------------------\n",
    "This is just a simple agent that uses the network but does not train it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import Agent\n",
    "class SimpleNetworkAgent(Agent):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.network = LanderNetwork()\n",
    "    def act(self, observation, periphral=None):\n",
    "        obs = torch.from_numpy(observation)\n",
    "        action = self.network(obs)\n",
    "        return action.argmax().item()\n",
    "    def record_observation(self, observation_old, action, reward, observation, terminated):\n",
    "        return super().record_observation(observation_old, action, reward, observation, terminated)\n",
    "\n",
    "sna = SimpleNetworkAgent()\n",
    "cont = None\n",
    "runs = [[]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_one_run\n",
    "\n",
    "plot_one_run(env, sna,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()\n",
    "# plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "env = gym.make(\"LunarLander-v3\")\n",
    "cont = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [00:09<00:00,  8.01it/s]\n"
     ]
    }
   ],
   "source": [
    "from utils import run_upto_n_steps, plot_reward_and_episodes\n",
    "from tqdm import trange\n",
    "\n",
    "for round in trange(80):\n",
    "    cont, runs = run_upto_n_steps(env, sna, 150, cont, runs)\n",
    "    if round % 2 == 0:\n",
    "        plt.clf()\n",
    "        plot_reward_and_episodes(runs)\n",
    "        plt.pause(0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainable Network Agent\n",
    "-----------------------\n",
    "This agent will train the network by optimizing on the most recent few \n",
    "(Observaation, Action, Reward, Observation) quartuples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "env = gym.make(\"LunarLander-v3\")\n",
    "cont = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils import Agent\n",
    "class TrainableNetworkAgent(Agent):\n",
    "    def __init__(self, gamma = 0.9, lr=0.001, update_interval=32, epsilon=0.1) -> None:\n",
    "        super().__init__()\n",
    "        self.network = LanderNetwork()\n",
    "        self.gamma = gamma\n",
    "        self.update_interval = update_interval\n",
    "        self.epsilon = epsilon\n",
    "        self.optim = torch.optim.AdamW(self.network.parameters(), lr)\n",
    "        self.loss = torch.tensor(0., requires_grad=True)\n",
    "        self.thoughts = 0\n",
    "        self.losses = []\n",
    "        self.events = [] # List of dictionary of events\n",
    "    def act(self, observation, periphral=None):\n",
    "        if self.network.training and torch.rand((1,)).item() < self.epsilon:\n",
    "            return torch.randint(self.network.layers[-1].out_features, (1,)).item()\n",
    "        obs = torch.from_numpy(observation)\n",
    "        with torch.no_grad():\n",
    "            action = self.network(obs)\n",
    "        return action.argmax().item()\n",
    "    def record_observation(self, observation, action, reward, observation_next, terminated):\n",
    "        if not self.network.training:\n",
    "            return \n",
    "        self.thoughts += 1\n",
    "        if isinstance(observation, np.ndarray):\n",
    "            observation = torch.from_numpy(observation)\n",
    "        if isinstance(observation_next, np.ndarray):\n",
    "            observation_next = torch.from_numpy(observation_next)\n",
    "        outputs = self.network(observation)\n",
    "        cr_pred = outputs[action]\n",
    "        cr_esti = reward + (self.gamma*self.network(observation_next).argmax() if not terminated else 0)\n",
    "\n",
    "        loss = (cr_esti - cr_pred)**2\n",
    "        self.loss = self.loss + loss\n",
    "        self.losses.append(loss.item())\n",
    "        self.events.append({\n",
    "            \"terminal\": terminated,\n",
    "            \"reward\": reward\n",
    "        })\n",
    "        if self.thoughts % self.update_interval == 0:\n",
    "            self.update()\n",
    "        \n",
    "    def update(self):\n",
    "        self.optim.zero_grad()\n",
    "        self.loss.backward()\n",
    "        self.optim.step()\n",
    "        self.loss = torch.tensor(0., requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tna = TrainableNetworkAgent(gamma=0.99, lr=0.01)\n",
    "cont = None\n",
    "runs = [[]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "thoughts 380525: : 200268it [07:22, 452.17it/s]                         \n"
     ]
    }
   ],
   "source": [
    "from utils import run_upto_n_steps, plot_reward_and_episodes\n",
    "from tqdm import trange\n",
    "from itertools import count\n",
    "tna.network.train()\n",
    "training_steps  = 20_000\n",
    "progbar = trange(training_steps)\n",
    "\n",
    "initial_steps = tna.thoughts\n",
    "breakout = False\n",
    "for round in count():\n",
    "    try:\n",
    "        cont, runs = run_upto_n_steps(env, tna, 150, cont, runs)\n",
    "        progbar.set_description(f\"thoughts {tna.thoughts}\")\n",
    "        progbar.update(tna.thoughts-initial_steps-progbar.n)\n",
    "        if progbar.n-initial_steps >= training_steps:\n",
    "            breakout = True\n",
    "        if round % 20 == 0 or breakout:\n",
    "            # plot_agent(tna)\n",
    "            plt.clf()\n",
    "            plt.subplot(2,1,1)\n",
    "            plt.scatter(*zip(*enumerate(tna.losses)), c=[i['terminal'] for i in tna.events], s=120, alpha=0.8)\n",
    "            plt.subplot(2, 1, 2)\n",
    "            plot_reward_and_episodes(runs)\n",
    "            plt.pause(0.2)\n",
    "        if round % 300 == 0 or breakout:\n",
    "            tna.network.eval()\n",
    "            enviz = gym.make(\"LunarLander-v3\", render_mode='human')\n",
    "            run_upto_n_steps(enviz, tna, 1_000)\n",
    "            enviz.close()\n",
    "            tna.network.train()\n",
    "        if breakout:\n",
    "            break\n",
    "    except KeyboardInterrupt:\n",
    "        break\n",
    "progbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_one_run\n",
    "enviz = gym.make(\"LunarLander-v3\", render_mode='human')\n",
    "tna.network.eval()\n",
    "plot_one_run(enviz, tna,)\n",
    "enviz.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0.98, 'Simple DQN')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# plt.close()\n",
    "plt.suptitle(\"Simple DQN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Parallel environments\n",
    "\n",
    "This paper [Asynchronous Methods for Deep Reinforcement Learning](https://arxiv.org/abs/1602.01783) \n",
    "demonstrated a strategy of training the same agent in multiple different copies \n",
    "of the environment at the same time as a method to keep diversity in training \n",
    "data.\n",
    "\n",
    "This section is my attempt to recreate that\n",
    "\n",
    "\n",
    "steps:\n",
    "- ~~Seperate runs from agent, make run an array returned along side cont~~\n",
    "- ~~Verify nothing broke~~\n",
    "- Implement env&run swap in training loop\n",
    "- ...?\n",
    "- Profit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import TrainableNetworkAgent\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "tna = TrainableNetworkAgent(gamma=0.99, lr=0.01)\n",
    "envs = [gym.make(\"LunarLander-v3\") for _ in range(5)] \n",
    "num_envs = len(envs)\n",
    "cont = [None]*num_envs\n",
    "runs = [ [[]] ]*num_envs\n",
    "# epsilons = np.linspace(0.9, 0.1, num_envs)\n",
    "epsilons = [0.1]*num_envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "thoughts 80028: 100%|█████████▉| 79988/80000 [03:14<00:00, 214.38it/s]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method tqdm.close of <tqdm.std.tqdm object at 0x7ee1a8aed450>>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import run_upto_n_steps, plot_reward_and_episodes\n",
    "from tqdm import trange\n",
    "from itertools import count\n",
    "\n",
    "tna.network.train()\n",
    "training_steps  = 80_000\n",
    "progbar = trange(training_steps)\n",
    "\n",
    "initial_steps = tna.thoughts\n",
    "breakout = False\n",
    "for round in count():\n",
    "    try:\n",
    "        env = round%num_envs\n",
    "        tna.epsilon = epsilons[env]\n",
    "        cont[env], runs[env] = run_upto_n_steps(envs[env], tna, 40, cont[env], runs[env])\n",
    "        progbar.set_description(f\"thoughts {tna.thoughts}\")\n",
    "        progbar.update(tna.thoughts-initial_steps-progbar.n)\n",
    "        if progbar.n-initial_steps >= training_steps:\n",
    "            breakout = True\n",
    "        if round % 20 == 0:\n",
    "            # plot_agent(tna)\n",
    "            plt.clf()\n",
    "            plt.subplot(2,1,1)\n",
    "            plt.scatter(*zip(*enumerate(tna.losses)), c=[i['terminal'] for i in tna.events], s=10, alpha=0.8)\n",
    "            plt.subplot(2, 1, 2)\n",
    "            plot_reward_and_episodes(runs[-1])\n",
    "            plt.pause(0.2)\n",
    "        if round % 500 == 0:\n",
    "            tna.network.eval()\n",
    "            enviz = gym.make(\"LunarLander-v3\", render_mode='human')\n",
    "            run_upto_n_steps(enviz, tna, 1_000)\n",
    "            enviz.close()\n",
    "            tna.network.train()\n",
    "        if breakout:\n",
    "            break\n",
    "    except KeyboardInterrupt:\n",
    "        break\n",
    "progbar.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:21<00:00,  4.39s/it]                         \n"
     ]
    }
   ],
   "source": [
    "enviz = gym.make(\"LunarLander-v3\", render_mode='human')\n",
    "tna.network.eval()\n",
    "for i in trange(5):\n",
    "    run_upto_n_steps(enviz, tna, 1_000)\n",
    "# input()\n",
    "enviz.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib tk\n",
    "# %matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import Agent, run_upto_n_steps\n",
    "import numpy as np\n",
    "class RandLander(Agent):\n",
    "    def act(self, observation, periphral=None):\n",
    "        return np.random.choice([0,1,2,3])\n",
    "    def record_observation(self, observation_old, action, reward, observation, terminated):\n",
    "        pass\n",
    "\n",
    "rand_agent = RandLander()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import Agent, run_upto_n_steps\n",
    "import gymnasium as gym\n",
    "\n",
    "env = gym.make(\"LunarLander-v3\", render_mode=\"human\")\n",
    "cont = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "# logging.basicConfig(level=logging.INFO) # If your interested in some logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import run_and_plot\n",
    "\n",
    "for round in range(100):\n",
    "    cont = run_and_plot(env, rand_agent, 50, cont)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "env = gym.make(\"LunarLander-v3\")\n",
    "cont = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import run_and_plot\n",
    "\n",
    "for round in range(100):\n",
    "    cont = run_and_plot(env, rand_agent, 150, cont)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "env = gym.make(\"LunarLander-v3\", render_mode=\"human\")\n",
    "observation, info = env.reset()\n",
    "\n",
    "episode_over = False\n",
    "while not episode_over:\n",
    "    action = env.action_space.sample()  # agent policy that uses the observation and info\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    episode_over = terminated or truncated\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define a function that takes a model and an environment, and run the model in the environment for *n* steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class Agent(ABC):\n",
    "    @abstractmethod\n",
    "    def act(self, observation, periphral=None):\n",
    "        pass\n",
    "    @abstractmethod\n",
    "    def think(self, observation_old, action, reward, observation):\n",
    "        pass\n",
    "\n",
    "def run_n_steps(env, agent: Agent, n, continuation=None):\n",
    "    if continuation is not None:\n",
    "        observation, reward, terminated, truncated, info = continuation\n",
    "    if continuation is None or terminated or truncated:\n",
    "        print(\"Resetting\")\n",
    "        observation, info = env.reset()\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        reward = 0\n",
    "    step = 0\n",
    "    # print((observation, reward, terminated, truncated, info))\n",
    "    while not terminated and not truncated and step < n:\n",
    "        action = agent.act(observation)\n",
    "        observation_old = observation\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        agent.think(observation_old, action, reward, observation)\n",
    "        step += 1\n",
    "    return (observation, reward, terminated, truncated, info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v3\", render_mode='human')\n",
    "\n",
    "class RandomLunar(Agent):\n",
    "    def act(self, observation, periphral=None):\n",
    "        return env.action_space.sample()\n",
    "    def think(self, observation_old, action, reward, observation):\n",
    "        return super().think(observation_old, action, reward, observation)\n",
    "randAgent = RandomLunar()\n",
    "\n",
    "\n",
    "cont = run_n_steps(env, randAgent, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont = run_n_steps(env, randAgent, 10, cont)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

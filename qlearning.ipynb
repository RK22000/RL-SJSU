{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib tk\n",
    "# %matplotlib qt\n",
    "import matplotlib.pyplot as plt\n",
    "plt.xkcd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-Learning Agent (Jax/Flax)\n",
    "==========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "from flax import nnx\n",
    "\n",
    "class LanderNetwork(nnx.Module):\n",
    "    def __init__(self, layers=None, rngs: nnx.Rngs=None) -> None:\n",
    "        if rngs is None: rngs = nnx.Rngs(0)\n",
    "        if layers is None: layers = [8, 128, 128, 4]\n",
    "        layers = [[nnx.Linear(i, o, rngs=rngs), nnx.relu] for i, o in zip(layers[:-1], layers[1:])]\n",
    "        layers = sum(layers, [])[:-1]\n",
    "        self.sequential = nnx.Sequential(\n",
    "            *layers\n",
    "            # nnx.Linear(8, 128, rngs=rngs),\n",
    "            # nnx.relu,\n",
    "            # nnx.Linear(128, 128, rngs=rngs),\n",
    "            # nnx.relu,\n",
    "            # nnx.Linear(128, 4, rngs=rngs)\n",
    "        )\n",
    "    def __call__(self, x):\n",
    "        return self.sequential(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "ln = LanderNetwork()\n",
    "ln(np.random.randn(8)).argmax(-1).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnx.display(LanderNetwork([8, 512, 256, 4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import Agent\n",
    "from collections import deque\n",
    "import optax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "class TrainableAgent(Agent):\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        network: nnx.Module, \n",
    "        buffer_size=32, batch_size=32,\n",
    "        gamma=0.9, \n",
    "        lr=0.001, \n",
    "        train_interval=4, \n",
    "        epsilon=0.1\n",
    "        ) -> None:\n",
    "        \n",
    "        self.network = network\n",
    "        self._training = False\n",
    "        self.buffer = deque([], maxlen=buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.train_interval = train_interval\n",
    "        self.training_steps = 0\n",
    "        self.gamma = gamma\n",
    "        self.losses = []\n",
    "        self.epsilon=epsilon\n",
    "        \n",
    "        self.optim = nnx.Optimizer(self.network, optax.adamw(lr,))\n",
    "        # self.gradient_on_samples = nnx.jit(nnx.grad(TrainableAgent.evaluate_samples))\n",
    "        self.gradient_on_samples = nnx.jit(nnx.grad(TrainableAgent.batch_evaluate_samples))\n",
    "        \n",
    "        \n",
    "    def __call__(self, x) -> np.ndarray:\n",
    "        return self.network(x)\n",
    "    def train(self):\n",
    "        self.network.train()\n",
    "        self._training=True\n",
    "        return self\n",
    "    def eval(self):\n",
    "        self.network.eval()\n",
    "        self._training=False\n",
    "        return self\n",
    "    def act(self, observation, periphral=None):\n",
    "        if self._training and np.random.rand() < self.epsilon:\n",
    "            return self._explore(observation)\n",
    "        return self._exploit(observation)\n",
    "    def _explore(self, observation):\n",
    "        return np.random.randint(4)\n",
    "    def _exploit(self, observation):\n",
    "        return self.network(observation).argmax(-1).item()\n",
    "    \n",
    "    def record_observation(self, observation, action, reward, next_observation, terminated):\n",
    "        if not self._training:\n",
    "            return\n",
    "        self.buffer.append((observation, action, reward, next_observation, int(terminated)))\n",
    "        self.training_steps+=1\n",
    "        self.losses.append(self.evaluate_sample(self.network, self.gamma, *self.buffer[-1]))\n",
    "        \n",
    "        if self.training_steps%self.train_interval==0 and len(self.buffer) > self.batch_size:\n",
    "            batch = [self.buffer[i] for i in np.random.choice(np.arange(len(self.buffer)), self.batch_size)]\n",
    "            self.train_on_samples(self.network, batch)\n",
    "\n",
    "    def train_on_samples(self, network, samples):\n",
    "        grads = self.gradient_on_samples(network, self.gamma, samples)\n",
    "        self.optim.update(grads)\n",
    "    \n",
    "    @staticmethod\n",
    "    def evaluate_samples(network, gamma, samples):\n",
    "        return sum((TrainableAgent.evaluate_sample(network, gamma, *sample) for sample in samples))\n",
    "    \n",
    "    @staticmethod\n",
    "    def batch_evaluate_samples(network: nnx.Module, gamma:int, samples):\n",
    "        observations, actions, rewards, next_observations, terminateds = zip(*samples)\n",
    "        terminateds = jnp.array(terminateds)\n",
    "        rewards = jnp.array(rewards)\n",
    "        cr_preds = jnp.stack([pred[a] for pred, a in zip(network(observations), actions)])\n",
    "        cr_estimates = rewards + (1-terminateds)*gamma*network(next_observations).argmax(-1)\n",
    "        return sum((cr_preds-cr_estimates)**2)\n",
    "\n",
    "    @staticmethod\n",
    "    def evaluate_sample(network, gamma, observation, action, reward, next_observation, terminated):\n",
    "        cr_pred = network(observation)[action]\n",
    "        cr_estimate = reward + (1-terminated)*gamma*network(next_observation).argmax(-1)\n",
    "        return (cr_pred-cr_estimate)**2\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single demo run\n",
    "---------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_one_run\n",
    "import gymnasium as gym\n",
    "enviz = gym.make(\"LunarLander-v3\", render_mode='human')\n",
    "tna = TrainableAgent(LanderNetwork()).eval()\n",
    "plot_one_run(enviz, tna, )\n",
    "enviz.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()\n",
    "enviz.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training\n",
    "--------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import gymnasium as gym\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "tna = TrainableAgent(LanderNetwork([8, 256, 256, 4]), buffer_size=50_000, gamma=0.99, lr=0.0005, batch_size=32, train_interval=1)\n",
    "cont = None\n",
    "runs = [[]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import run_upto_n_steps, plot_reward_and_episodes, plot_one_run\n",
    "from tqdm import trange\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_losses_and_runs(losses, runs):\n",
    "    plt.clf()\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.scatter(*zip(*enumerate(losses)), )\n",
    "    plt.yscale('log')\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plot_reward_and_episodes(runs[:-1]) if len(runs) > 1 else None\n",
    "    plt.pause(0.5)\n",
    "\n",
    "def demo_one_run(agent: Agent):\n",
    "    plt.clf()\n",
    "    agent.eval()\n",
    "    enviz = gym.make(\"LunarLander-v2\", render_mode='human')\n",
    "    plot_one_run(enviz, agent, )\n",
    "    enviz.close()\n",
    "    agent.train()\n",
    "tna.train()\n",
    "\n",
    "step_limp = tna.training_steps + 100_000\n",
    "prog_bar = trange(tna.training_steps, step_limp)\n",
    "plots=0\n",
    "demos=0\n",
    "\n",
    "try:\n",
    "    while tna.training_steps < step_limp:\n",
    "        init_steps = tna.training_steps\n",
    "        cont, runs = run_upto_n_steps(env, tna, 1_000, cont, runs)\n",
    "        prog_bar.update(np.min([tna.training_steps, step_limp]) - init_steps, )\n",
    "\n",
    "        if tna.training_steps // 5_000 > demos:\n",
    "            demo_one_run(tna)\n",
    "            demos = tna.training_steps//5_000\n",
    "\n",
    "        if tna.training_steps // 500 > plots:\n",
    "            plot_losses_and_runs(tna.losses, runs)\n",
    "            plots = tna.training_steps//500\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "prog_bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "prog_bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tna.training_steps, len(tna.losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_one_run(tna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses_and_runs(tna.losses, runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from flax.training.train_state import TrainState\n",
    "from flax.serialization import to_state_dict\n",
    "\n",
    "# to_state_dict(tna.network.params)\n",
    "tna.network.params\n",
    "\n",
    "\n",
    "# state = TrainState.create(\n",
    "    # apply_fn=tna.network.__call__\n",
    "# )\n",
    "# tna.network.__call__\n",
    "# tna.network\n",
    "# tna.optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnx.state(tna.network)\n",
    "# nnx.split(tna.network)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-Learning Agent (pytorch)\n",
    "=========================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "setup\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "enviz = gym.make(\"LunarLander-v3\", render_mode='human')\n",
    "cont = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enviz.observation_space.sample().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enviz.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LanderNetwork(nn.Module):\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(8, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 4),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Network Agent\n",
    "--------------------\n",
    "This is just a simple agent that uses the network but does not train it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import Agent\n",
    "class SimpleNetworkAgent(Agent):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.network = LanderNetwork()\n",
    "    def act(self, observation, periphral=None):\n",
    "        obs = torch.from_numpy(observation)\n",
    "        action = self.network(obs)\n",
    "        return action.argmax().item()\n",
    "    def record_observation(self, observation_old, action, reward, observation, terminated):\n",
    "        return super().record_observation(observation_old, action, reward, observation, terminated)\n",
    "\n",
    "sna = SimpleNetworkAgent()\n",
    "cont = None\n",
    "runs = [[]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_one_run\n",
    "\n",
    "plot_one_run(enviz, sna,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "enviz.close()\n",
    "# plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "enviz = gym.make(\"LunarLander-v3\")\n",
    "cont = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import run_upto_n_steps, plot_reward_and_episodes\n",
    "from tqdm import trange\n",
    "\n",
    "for round in trange(80):\n",
    "    cont, runs = run_upto_n_steps(enviz, sna, 150, cont, runs)\n",
    "    if round % 2 == 0:\n",
    "        plt.clf()\n",
    "        plot_reward_and_episodes(runs)\n",
    "        plt.pause(0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainable Network Agent\n",
    "-----------------------\n",
    "This agent will train the network by optimizing on the most recent few \n",
    "(Observaation, Action, Reward, Observation) quartuples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "env = gym.make(\"LunarLander-v3\")\n",
    "cont = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils import Agent\n",
    "class TrainableNetworkAgent(Agent):\n",
    "    def __init__(self, gamma = 0.9, lr=0.001, update_interval=32, epsilon=0.1) -> None:\n",
    "        super().__init__()\n",
    "        self.network = LanderNetwork()\n",
    "        self.gamma = gamma\n",
    "        self.update_interval = update_interval\n",
    "        self.epsilon = epsilon\n",
    "        self.optim = torch.optim.AdamW(self.network.parameters(), lr)\n",
    "        self.loss = torch.tensor(0., requires_grad=True)\n",
    "        self.thoughts = 0\n",
    "        self.losses = []\n",
    "        self.events = [] # List of dictionary of events\n",
    "    def act(self, observation, periphral=None):\n",
    "        if self.network.training and torch.rand((1,)).item() < self.epsilon:\n",
    "            return torch.randint(self.network.layers[-1].out_features, (1,)).item()\n",
    "        obs = torch.from_numpy(observation)\n",
    "        with torch.no_grad():\n",
    "            action = self.network(obs)\n",
    "        return action.argmax().item()\n",
    "    def record_observation(self, observation, action, reward, observation_next, terminated):\n",
    "        if not self.network.training:\n",
    "            return \n",
    "        self.thoughts += 1\n",
    "        if isinstance(observation, np.ndarray):\n",
    "            observation = torch.from_numpy(observation)\n",
    "        if isinstance(observation_next, np.ndarray):\n",
    "            observation_next = torch.from_numpy(observation_next)\n",
    "        outputs = self.network(observation)\n",
    "        cr_pred = outputs[action]\n",
    "        cr_esti = reward + (self.gamma*self.network(observation_next).argmax() if not terminated else 0)\n",
    "\n",
    "        loss = (cr_esti - cr_pred)**2\n",
    "        self.loss = self.loss + loss\n",
    "        self.losses.append(loss.item())\n",
    "        self.events.append({\n",
    "            \"terminal\": terminated,\n",
    "            \"reward\": reward\n",
    "        })\n",
    "        if self.thoughts % self.update_interval == 0:\n",
    "            self.update()\n",
    "        \n",
    "    def update(self):\n",
    "        self.optim.zero_grad()\n",
    "        self.loss.backward()\n",
    "        self.optim.step()\n",
    "        self.loss = torch.tensor(0., requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "tna = TrainableNetworkAgent(gamma=0.99, lr=0.01)\n",
    "cont = None\n",
    "runs = [[]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import run_upto_n_steps, plot_reward_and_episodes\n",
    "from tqdm import trange\n",
    "from itertools import count\n",
    "tna.network.train()\n",
    "training_steps  = 10_000\n",
    "progbar = trange(training_steps)\n",
    "\n",
    "initial_steps = tna.thoughts\n",
    "breakout = False\n",
    "for round in count():\n",
    "    try:\n",
    "        cont, runs = run_upto_n_steps(env, tna, 150, cont, runs)\n",
    "        progbar.set_description(f\"thoughts {tna.thoughts}\")\n",
    "        progbar.update(tna.thoughts-initial_steps-progbar.n)\n",
    "        if progbar.n-initial_steps >= training_steps:\n",
    "            breakout = True\n",
    "        if round % 20 == 0 or breakout:\n",
    "            # plot_agent(tna)\n",
    "            plt.clf()\n",
    "            plt.subplot(2,1,1)\n",
    "            plt.scatter(*zip(*enumerate(tna.losses)), c=[i['terminal'] for i in tna.events], s=120, alpha=0.8)\n",
    "            plt.yscale('log')\n",
    "            plt.subplot(2, 1, 2)\n",
    "            plot_reward_and_episodes(runs)\n",
    "            plt.pause(0.2)\n",
    "        if round % 300 == 0 or breakout:\n",
    "            tna.network.eval()\n",
    "            enviz = gym.make(\"LunarLander-v3\", render_mode='human')\n",
    "            run_upto_n_steps(enviz, tna, 1_000)\n",
    "            enviz.close()\n",
    "            tna.network.train()\n",
    "        if breakout:\n",
    "            break\n",
    "    except KeyboardInterrupt:\n",
    "        break\n",
    "progbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_one_run\n",
    "enviz = gym.make(\"LunarLander-v3\", render_mode='human')\n",
    "tna.network.eval()\n",
    "plot_one_run(enviz, tna,)\n",
    "enviz.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.close()\n",
    "plt.suptitle(\"Simple DQN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Parallel environments\n",
    "\n",
    "This paper [Asynchronous Methods for Deep Reinforcement Learning](https://arxiv.org/abs/1602.01783) \n",
    "demonstrated a strategy of training the same agent in multiple different copies \n",
    "of the environment at the same time as a method to keep diversity in training \n",
    "data.\n",
    "\n",
    "This section is my attempt to recreate that\n",
    "\n",
    "\n",
    "steps:\n",
    "- ~~Seperate runs from agent, make run an array returned along side cont~~\n",
    "- ~~Verify nothing broke~~\n",
    "- Implement env&run swap in training loop\n",
    "- ...?\n",
    "- Profit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import TrainableNetworkAgent\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "tna = TrainableNetworkAgent(gamma=0.99, lr=0.01)\n",
    "envs = [gym.make(\"LunarLander-v3\") for _ in range(5)] \n",
    "num_envs = len(envs)\n",
    "cont = [None]*num_envs\n",
    "runs = [ [[]] ]*num_envs\n",
    "# epsilons = np.linspace(0.9, 0.1, num_envs)\n",
    "epsilons = [0.1]*num_envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import run_upto_n_steps, plot_reward_and_episodes\n",
    "from tqdm import trange\n",
    "from itertools import count\n",
    "\n",
    "tna.network.train()\n",
    "training_steps  = 80_000\n",
    "progbar = trange(training_steps)\n",
    "\n",
    "initial_steps = tna.thoughts\n",
    "breakout = False\n",
    "for round in count():\n",
    "    try:\n",
    "        enviz = round%num_envs\n",
    "        tna.epsilon = epsilons[enviz]\n",
    "        cont[enviz], runs[enviz] = run_upto_n_steps(envs[enviz], tna, 40, cont[enviz], runs[enviz])\n",
    "        progbar.set_description(f\"thoughts {tna.thoughts}\")\n",
    "        progbar.update(tna.thoughts-initial_steps-progbar.n)\n",
    "        if progbar.n-initial_steps >= training_steps:\n",
    "            breakout = True\n",
    "        if round % 20 == 0:\n",
    "            # plot_agent(tna)\n",
    "            plt.clf()\n",
    "            plt.subplot(2,1,1)\n",
    "            plt.scatter(*zip(*enumerate(tna.losses)), c=[i['terminal'] for i in tna.events], s=10, alpha=0.8)\n",
    "            plt.subplot(2, 1, 2)\n",
    "            plot_reward_and_episodes(runs[-1])\n",
    "            plt.pause(0.2)\n",
    "        if round % 500 == 0:\n",
    "            tna.network.eval()\n",
    "            enviz = gym.make(\"LunarLander-v3\", render_mode='human')\n",
    "            run_upto_n_steps(enviz, tna, 1_000)\n",
    "            enviz.close()\n",
    "            tna.network.train()\n",
    "        if breakout:\n",
    "            break\n",
    "    except KeyboardInterrupt:\n",
    "        break\n",
    "progbar.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enviz = gym.make(\"LunarLander-v3\", render_mode='human')\n",
    "tna.network.eval()\n",
    "for i in trange(5):\n",
    "    run_upto_n_steps(enviz, tna, 1_000)\n",
    "# input()\n",
    "enviz.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib tk\n",
    "# %matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import Agent, run_upto_n_steps\n",
    "import numpy as np\n",
    "class RandLander(Agent):\n",
    "    def act(self, observation, periphral=None):\n",
    "        return np.random.choice([0,1,2,3])\n",
    "    def record_observation(self, observation_old, action, reward, observation, terminated):\n",
    "        pass\n",
    "\n",
    "rand_agent = RandLander()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import Agent, run_upto_n_steps\n",
    "import gymnasium as gym\n",
    "\n",
    "enviz = gym.make(\"LunarLander-v3\", render_mode=\"human\")\n",
    "cont = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "# logging.basicConfig(level=logging.INFO) # If your interested in some logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import run_and_plot\n",
    "\n",
    "for round in range(100):\n",
    "    cont = run_and_plot(enviz, rand_agent, 50, cont)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "enviz.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "enviz = gym.make(\"LunarLander-v3\")\n",
    "cont = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import run_and_plot\n",
    "\n",
    "for round in range(100):\n",
    "    cont = run_and_plot(enviz, rand_agent, 150, cont)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "enviz = gym.make(\"LunarLander-v3\", render_mode=\"human\")\n",
    "next_observation, info = enviz.reset()\n",
    "\n",
    "episode_over = False\n",
    "while not episode_over:\n",
    "    action = enviz.action_space.sample()  # agent policy that uses the observation and info\n",
    "    next_observation, reward, terminated, truncated, info = enviz.step(action)\n",
    "\n",
    "    episode_over = terminated or truncated\n",
    "\n",
    "enviz.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define a function that takes a model and an environment, and run the model in the environment for *n* steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class Agent(ABC):\n",
    "    @abstractmethod\n",
    "    def act(self, observation, periphral=None):\n",
    "        pass\n",
    "    @abstractmethod\n",
    "    def think(self, observation_old, action, reward, observation):\n",
    "        pass\n",
    "\n",
    "def run_n_steps(env, agent: Agent, n, continuation=None):\n",
    "    if continuation is not None:\n",
    "        observation, reward, terminated, truncated, info = continuation\n",
    "    if continuation is None or terminated or truncated:\n",
    "        print(\"Resetting\")\n",
    "        observation, info = env.reset()\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        reward = 0\n",
    "    step = 0\n",
    "    # print((observation, reward, terminated, truncated, info))\n",
    "    while not terminated and not truncated and step < n:\n",
    "        action = agent.act(observation)\n",
    "        observation_old = observation\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        agent.think(observation_old, action, reward, observation)\n",
    "        step += 1\n",
    "    return (observation, reward, terminated, truncated, info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enviz = gym.make(\"LunarLander-v3\", render_mode='human')\n",
    "\n",
    "class RandomLunar(Agent):\n",
    "    def act(self, observation, periphral=None):\n",
    "        return enviz.action_space.sample()\n",
    "    def think(self, observation_old, action, reward, observation):\n",
    "        return super().think(observation_old, action, reward, observation)\n",
    "randAgent = RandomLunar()\n",
    "\n",
    "\n",
    "cont = run_n_steps(enviz, randAgent, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont = run_n_steps(enviz, randAgent, 10, cont)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "enviz.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
